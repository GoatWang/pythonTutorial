{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/jeremy/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('universal_tagset')\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import json # 讀寫json文件\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# file = open('文檔地址', 'w/r', encoding='utf8')\n",
    "# mydict = json.load(file.read(), file)\n",
    "\n",
    "# json.dumps(mydicd, 'path', ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Knowledge\n",
    "- Tokenize: split words\n",
    "- Stemming: normalize words\n",
    "- remove punctuation\n",
    "- remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
      "['EVRY', 'is', 'one', 'of', 'the', 'leading', 'IT', 'companies', 'in', 'the', 'Nordic', 'region', '.']\n",
      "['EVRY', 'is', 'one', 'of', 'the', 'leading', 'IT', 'companies', 'in', 'the', 'Nordic', 'region.']\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)\n",
    "print(stops)\n",
    "print(nltk.word_tokenize(\"EVRY is one of the leading IT companies in the Nordic region.\"))\n",
    "print(\"EVRY is one of the leading IT companies in the Nordic region.\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "487\n"
     ]
    }
   ],
   "source": [
    "testStr = \"\"\"EVRY is one of the leading IT companies in the Nordic region. Through ideas, technology and solutions, EVRY brings information to life, creating value for its customers' businesses and making a positive contribution to society. EVRY combines extensive industry experience with a customer centric approach, and international capabilities with local presence to help customers realize the full potential of IT. We operate in industries such as finance, health, insurance, public, telecoms, oil and gas. \"\"\"\n",
    "\n",
    "# Please define a function remove punctuations\n",
    "def remove_punctuation(text):\n",
    "### Start your code here(~~~two line~~~)\n",
    "    text = testStr\n",
    "    for i in string.punctuation:\n",
    "        text = text.replace(i, '')\n",
    "### End your code here\n",
    "    return text\n",
    "\n",
    "print(len(testStr))\n",
    "testStr = remove_punctuation(testStr)\n",
    "print(len(testStr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expected return: \n",
    "\n",
    "</br>501\n",
    "\n",
    "</br>487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please tokenize the sentence in terms\n",
    "### Start your code here(~~~one line~~~)\n",
    "terms = nltk.word_tokenize(testStr)\n",
    "### End your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EVRY is one of the leading IT companies in the Nordic region Through ideas technology and solutions EVRY brings information to life creating value for its customers businesses and making a positive contribution to society EVRY combines extensive industry experience with a customer centric approach and international capabilities with local presence to help customers realize the full potential of IT We operate in industries such as finance health insurance public telecoms oil and gas'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "newterms = []\n",
    "for term in terms:\n",
    "    term = stemmer.stem(term)\n",
    "    if term not in stops:\n",
    "        newterms.append(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': ['what time is it?', 'how long has it been since we started?', \"that's a long time ago\", ' I spoke to you last week', ' I saw you yesterday'], 'sorry': [\"I'm extremely sorry\", 'did he apologize to you?', \"I shouldn't have been rude\"], 'greeting': ['Hello there!', 'Hey man! How are you?', 'hi'], 'farewell': ['It was a pleasure meeting you', 'Good Bye.', 'see you soon', 'I gotta go now.'], 'age': [\"what's your age?\", 'How old are you?', \"I'm a couple of years older than her\", 'You look aged!']}\n"
     ]
    }
   ],
   "source": [
    "# read the json file and load the training data\n",
    "import os\n",
    "\n",
    "# os.path.join('datasets', 'text.json') = 'datasets'\\\\'text.json'(windows), 'datasets'/'text.json'(mac)\n",
    "with open(os.path.join('datasets', 'text.json')) as json_data:\n",
    "    data = json.load(json_data)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 2, 3, 4]\n",
      "[1, 2, 3, [2, 3, 4]]\n",
      "[2, 3, 4]\n",
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# print(data.items())\n",
    "\n",
    "AA = [1,2,3]\n",
    "AA.extend([2,3,4])\n",
    "print(AA)\n",
    "\n",
    "AA = [1,2,3]\n",
    "AA.append([2,3,4])\n",
    "print(AA)\n",
    "print(AA[3])\n",
    "\n",
    "\n",
    "AA = [1,2,3]\n",
    "AA.extend([2,3,4])\n",
    "print(list(set(AA)))\n",
    "\n",
    "# type({1,2,3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ag', 'ago', 'apolog', 'ar', 'been', 'bye', 'coupl', 'did', 'extrem', 'go', 'good', 'got', 'has', 'hav', 'he', 'hello', 'her', 'hey', 'hi', 'how', 'i', 'im', 'is', 'it', 'last', 'long', 'look', 'man', 'meet', 'now', 'of', 'old', 'pleas', 'rud', 'saw', 'see', 'shouldnt', 'sint', 'soon', 'sorry', 'spok', 'start', 'ta', 'than', 'that', 'ther', 'tim', 'to', 'was', 'we', 'week', 'what', 'year', 'yesterday', 'yo', 'you']\n",
      "[(['what', 'time', 'is', 'it'], 'time'),\n",
      " (['how', 'long', 'has', 'it', 'been', 'since', 'we', 'started'], 'time'),\n",
      " (['thats', 'a', 'long', 'time', 'ago'], 'time'),\n",
      " (['I', 'spoke', 'to', 'you', 'last', 'week'], 'time'),\n",
      " (['I', 'saw', 'you', 'yesterday'], 'time'),\n",
      " (['Im', 'extremely', 'sorry'], 'sorry'),\n",
      " (['did', 'he', 'apologize', 'to', 'you'], 'sorry'),\n",
      " (['I', 'shouldnt', 'have', 'been', 'rude'], 'sorry'),\n",
      " (['Hello', 'there'], 'greeting'),\n",
      " (['Hey', 'man', 'How', 'are', 'you'], 'greeting'),\n",
      " (['hi'], 'greeting'),\n",
      " (['It', 'was', 'a', 'pleasure', 'meeting', 'you'], 'farewell'),\n",
      " (['Good', 'Bye'], 'farewell'),\n",
      " (['see', 'you', 'soon'], 'farewell'),\n",
      " (['I', 'got', 'ta', 'go', 'now'], 'farewell'),\n",
      " (['whats', 'your', 'age'], 'age'),\n",
      " (['How', 'old', 'are', 'you'], 'age'),\n",
      " (['Im', 'a', 'couple', 'of', 'years', 'older', 'than', 'her'], 'age'),\n",
      " (['You', 'look', 'aged'], 'age')]\n"
     ]
    }
   ],
   "source": [
    "# method to remove punctuations from sentences.\n",
    "def remove_punctuation(text):\n",
    "    for pun in string.punctuation:\n",
    "        text = text.replace(pun, \"\") \n",
    "    return text\n",
    "\n",
    "#initialize the stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# get a list of all categories to train for\n",
    "categories = list(data.keys())\n",
    "words = [] # 希望在整個資料及當中找出不重複的字，作為一條主向量\n",
    "# a list of tuples with words in the sentence and category name \n",
    "docs = [] # mapping 每一筆資料成為，（wordlist, label）,放到list中\n",
    "\n",
    "for each_category, sentences in data.items():\n",
    "    for each_sentence in sentences:\n",
    "        # remove any punctuation from the sentence\n",
    "        each_sentence = remove_punctuation(each_sentence)\n",
    "#         print(each_sentence)\n",
    "        # extract words from each sentence and append to the word list\n",
    "        w = nltk.word_tokenize(each_sentence)\n",
    "#         print(\"tokenized words: \", w)\n",
    "        words.extend(w)\n",
    "        docs.append((w, each_category))\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "from pprint import pprint\n",
    "print (words)\n",
    "pprint (docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "mappingDict = {'age':0, 'farewell':1, 'greeting':2, 'sorry':3, 'time':4}\n",
    "for doc, category in docs:\n",
    "    docarray = [0] * len(words)\n",
    "    for term in doc:\n",
    "        if term in words:\n",
    "            idx = words.index(stemmer.stem(term.lower()))\n",
    "            docarray[idx] +=1\n",
    "    X.append(docarray)\n",
    "    Y.append(mappingDict.get(category))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 57)\n",
      "(19,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(X)\n",
    "print(x.shape)\n",
    "\n",
    "y = np.array(Y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 3, 3, 3, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'DET'),\n",
       " ('ag', 'NOUN'),\n",
       " ('ago', 'ADV'),\n",
       " ('apolog', 'VERB'),\n",
       " ('ar', 'ADV'),\n",
       " ('been', 'VERB'),\n",
       " ('bye', 'ADJ'),\n",
       " ('coupl', 'NOUN'),\n",
       " ('did', 'VERB'),\n",
       " ('extrem', 'ADV'),\n",
       " ('go', 'VERB'),\n",
       " ('good', 'ADJ'),\n",
       " ('got', 'NOUN'),\n",
       " ('has', 'VERB'),\n",
       " ('hav', 'VERB'),\n",
       " ('he', 'PRON'),\n",
       " ('hello', 'VERB'),\n",
       " ('her', 'PRON'),\n",
       " ('hey', 'NOUN'),\n",
       " ('hi', 'VERB'),\n",
       " ('how', 'ADV'),\n",
       " ('i', 'ADJ'),\n",
       " ('im', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('it', 'PRON'),\n",
       " ('last', 'ADJ'),\n",
       " ('long', 'ADJ'),\n",
       " ('look', 'NOUN'),\n",
       " ('man', 'NOUN'),\n",
       " ('meet', 'NOUN'),\n",
       " ('now', 'ADV'),\n",
       " ('of', 'ADP'),\n",
       " ('old', 'ADJ'),\n",
       " ('pleas', 'NOUN'),\n",
       " ('rud', 'VERB'),\n",
       " ('saw', 'VERB'),\n",
       " ('see', 'VERB'),\n",
       " ('shouldnt', 'ADJ'),\n",
       " ('sint', 'NOUN'),\n",
       " ('soon', 'ADV'),\n",
       " ('sorry', 'ADJ'),\n",
       " ('spok', 'ADJ'),\n",
       " ('start', 'NOUN'),\n",
       " ('ta', 'NOUN'),\n",
       " ('than', 'ADP'),\n",
       " ('that', 'DET'),\n",
       " ('ther', 'NOUN'),\n",
       " ('tim', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('was', 'VERB'),\n",
       " ('we', 'PRON'),\n",
       " ('week', 'NOUN'),\n",
       " ('what', 'PRON'),\n",
       " ('year', 'NOUN'),\n",
       " ('yesterday', 'NOUN'),\n",
       " ('yo', 'ADP'),\n",
       " ('you', 'PRON')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words)\n",
    "nltk.pos_tag(words, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytutorial",
   "language": "python",
   "name": "pytutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
