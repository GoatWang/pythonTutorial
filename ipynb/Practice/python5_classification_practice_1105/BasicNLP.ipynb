{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Knowledge\n",
    "- Tokenize: split words\n",
    "- Stemming: normalize words\n",
    "- remove punctuation\n",
    "- remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
      "['EVRY', 'is', 'one', 'of', 'the', 'leading', 'IT', 'companies', 'in', 'the', 'Nordic', 'region', '.']\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)\n",
    "print(stops)\n",
    "print(nltk.word_tokenize(\"EVRY is one of the leading IT companies in the Nordic region.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-3f22a0b05e7d>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-3f22a0b05e7d>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    for  in :\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "testStr = \"\"\"EVRY is one of the leading IT companies in the Nordic region. Through ideas, technology and solutions, EVRY brings information to life, creating value for its customers' businesses and making a positive contribution to society. EVRY combines extensive industry experience with a customer centric approach, and international capabilities with local presence to help customers realize the full potential of IT. We operate in industries such as finance, health, insurance, public, telecoms, oil and gas. \"\"\"\n",
    "\n",
    "# Please define a function remove punctuations\n",
    "def remove_punctuation(text):\n",
    "### Start your code here(~~~two line~~~)\n",
    "    for  in :\n",
    "        text =  \n",
    "### End your code here\n",
    "    return text\n",
    "\n",
    "print(len(testStr))\n",
    "testStr = remove_punctuation(testStr)\n",
    "print(len(testStr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expected return: \n",
    "\n",
    "</br>501\n",
    "\n",
    "</br>487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please tokenize the sentence in terms\n",
    "### Start your code here(~~~one line~~~)\n",
    "terms = \n",
    "### End your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "for term in terms:\n",
    "    if term not in stops:\n",
    "# Please stem the term\n",
    "### Start your code here(~~~one line~~~)\n",
    "        stemmed = \n",
    "### End your code here\n",
    "        print(term, stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': ['what time is it?', 'how long has it been since we started?', \"that's a long time ago\", ' I spoke to you last week', ' I saw you yesterday'], 'sorry': [\"I'm extremely sorry\", 'did he apologize to you?', \"I shouldn't have been rude\"], 'greeting': ['Hello there!', 'Hey man! How are you?', 'hi'], 'farewell': ['It was a pleasure meeting you', 'Good Bye.', 'see you soon', 'I gotta go now.'], 'age': [\"what's your age?\", 'How old are you?', \"I'm a couple of years older than her\", 'You look aged!']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# read the json file and load the training data\n",
    "with open(os.path.join('datasets', 'text.json')) as json_data:\n",
    "    data = json.load(json_data)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a pleasure meeting you\n",
      "tokenized words:  ['It', 'was', 'a', 'pleasure', 'meeting', 'you']\n",
      "Good Bye\n",
      "tokenized words:  ['Good', 'Bye']\n",
      "see you soon\n",
      "tokenized words:  ['see', 'you', 'soon']\n",
      "I gotta go now\n",
      "tokenized words:  ['I', 'got', 'ta', 'go', 'now']\n",
      "what time is it\n",
      "tokenized words:  ['what', 'time', 'is', 'it']\n",
      "how long has it been since we started\n",
      "tokenized words:  ['how', 'long', 'has', 'it', 'been', 'since', 'we', 'started']\n",
      "thats a long time ago\n",
      "tokenized words:  ['thats', 'a', 'long', 'time', 'ago']\n",
      " I spoke to you last week\n",
      "tokenized words:  ['I', 'spoke', 'to', 'you', 'last', 'week']\n",
      " I saw you yesterday\n",
      "tokenized words:  ['I', 'saw', 'you', 'yesterday']\n",
      "Im extremely sorry\n",
      "tokenized words:  ['Im', 'extremely', 'sorry']\n",
      "did he apologize to you\n",
      "tokenized words:  ['did', 'he', 'apologize', 'to', 'you']\n",
      "I shouldnt have been rude\n",
      "tokenized words:  ['I', 'shouldnt', 'have', 'been', 'rude']\n",
      "whats your age\n",
      "tokenized words:  ['whats', 'your', 'age']\n",
      "How old are you\n",
      "tokenized words:  ['How', 'old', 'are', 'you']\n",
      "Im a couple of years older than her\n",
      "tokenized words:  ['Im', 'a', 'couple', 'of', 'years', 'older', 'than', 'her']\n",
      "You look aged\n",
      "tokenized words:  ['You', 'look', 'aged']\n",
      "Hello there\n",
      "tokenized words:  ['Hello', 'there']\n",
      "Hey man How are you\n",
      "tokenized words:  ['Hey', 'man', 'How', 'are', 'you']\n",
      "hi\n",
      "tokenized words:  ['hi']\n",
      "['a', 'ag', 'ago', 'apolog', 'ar', 'been', 'bye', 'coupl', 'did', 'extrem', 'go', 'good', 'got', 'has', 'hav', 'he', 'hello', 'her', 'hey', 'hi', 'how', 'i', 'im', 'is', 'it', 'last', 'long', 'look', 'man', 'meet', 'now', 'of', 'old', 'pleas', 'rud', 'saw', 'see', 'shouldnt', 'sint', 'soon', 'sorry', 'spok', 'start', 'ta', 'than', 'that', 'ther', 'tim', 'to', 'was', 'we', 'week', 'what', 'year', 'yesterday', 'yo', 'you']\n",
      "[(['It', 'was', 'a', 'pleasure', 'meeting', 'you'], 'farewell'),\n",
      " (['Good', 'Bye'], 'farewell'),\n",
      " (['see', 'you', 'soon'], 'farewell'),\n",
      " (['I', 'got', 'ta', 'go', 'now'], 'farewell'),\n",
      " (['what', 'time', 'is', 'it'], 'time'),\n",
      " (['how', 'long', 'has', 'it', 'been', 'since', 'we', 'started'], 'time'),\n",
      " (['thats', 'a', 'long', 'time', 'ago'], 'time'),\n",
      " (['I', 'spoke', 'to', 'you', 'last', 'week'], 'time'),\n",
      " (['I', 'saw', 'you', 'yesterday'], 'time'),\n",
      " (['Im', 'extremely', 'sorry'], 'sorry'),\n",
      " (['did', 'he', 'apologize', 'to', 'you'], 'sorry'),\n",
      " (['I', 'shouldnt', 'have', 'been', 'rude'], 'sorry'),\n",
      " (['whats', 'your', 'age'], 'age'),\n",
      " (['How', 'old', 'are', 'you'], 'age'),\n",
      " (['Im', 'a', 'couple', 'of', 'years', 'older', 'than', 'her'], 'age'),\n",
      " (['You', 'look', 'aged'], 'age'),\n",
      " (['Hello', 'there'], 'greeting'),\n",
      " (['Hey', 'man', 'How', 'are', 'you'], 'greeting'),\n",
      " (['hi'], 'greeting')]\n"
     ]
    }
   ],
   "source": [
    "# method to remove punctuations from sentences.\n",
    "def remove_punctuation(text):\n",
    "    for pun in string.punctuation:\n",
    "        text = text.replace(pun, \"\") \n",
    "    return text\n",
    "\n",
    "#initialize the stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# get a list of all categories to train for\n",
    "categories = list(data.keys())\n",
    "words = []\n",
    "# a list of tuples with words in the sentence and category name \n",
    "docs = []\n",
    "\n",
    "for each_category, sentences in data.items():\n",
    "    for each_sentence in sentences:\n",
    "        # remove any punctuation from the sentence\n",
    "        each_sentence = remove_punctuation(each_sentence)\n",
    "        print(each_sentence)\n",
    "        # extract words from each sentence and append to the word list\n",
    "        w = nltk.word_tokenize(each_sentence)\n",
    "        print(\"tokenized words: \", w)\n",
    "        words.extend(w)\n",
    "        docs.append((w, each_category))\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "from pprint import pprint\n",
    "print (words)\n",
    "pprint (docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
